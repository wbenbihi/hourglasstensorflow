{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Parsing MPII Dataset\n",
    "\n",
    "- Date: 2022\n",
    "- Author: Walid BENBIHI\n",
    "- Source: [wbenbihi/hourglasstensorflow](https://github.com/wbenbihi/hourglasstensorflow) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Imports\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourglass Tensorflow Imports\n",
    "from hourglass_tensorflow.utils.parsers import mpii as mpii_parser\n",
    "\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAct\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIDataset\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAnnorect\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAnnoPoint\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIDatapoint\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAnnotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = '..'\n",
    "DATA_FOLDER = \"data\"\n",
    "MPII_MAT = \"mpii.ignore.mat\"\n",
    "MPII_FILE = os.path.join(ROOT_FOLDER, DATA_FOLDER, MPII_MAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPII Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------- \n",
    "MPII Human Pose Dataset, Version 1.0 \n",
    "Copyright 2015 Max Planck Institute for Informatics \n",
    "Licensed under the Simplified BSD License [see bsd.txt] \n",
    "--------------------------------------------------------------------------- \n",
    "\n",
    "We are making the annotations and the corresponding code freely available for research \n",
    "purposes. If you would like to use the dataset for any other purposes please contact \n",
    "the authors. \n",
    "\n",
    "### Introduction\n",
    "MPII Human Pose dataset is a state of the art benchmark for evaluation\n",
    "of articulated human pose estimation. The dataset includes around\n",
    "**25K images** containing over **40K people** with annotated body\n",
    "joints. The images were systematically collected using an established\n",
    "taxonomy of every day human activities. Overall the dataset covers\n",
    "**410 human activities** and each image assigned an activity\n",
    "label. Each image was extracted from a YouTube video and provided with\n",
    "preceding and following un-annotated frames. In addition, for the test\n",
    "set we obtained richer annotations including body part occlusions and\n",
    "3D torso and head orientations.\n",
    "\n",
    "Following the best practices for the performance evaluation benchmarks\n",
    "in the literature we withhold the test annotations to prevent\n",
    "overfitting and tuning on the test set. We are working on an automatic\n",
    "evaluation server and performance analysis tools based on rich test\n",
    "set annotations.\n",
    "\n",
    "### Citing the dataset\n",
    "```\n",
    "@inproceedings{andriluka14cvpr,\n",
    "               author = {Mykhaylo Andriluka and Leonid Pishchulin and Peter Gehler and Schiele, Bernt}\n",
    "               title = {2D Human Pose Estimation: New Benchmark and State of the Art Analysis},\n",
    "               booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "               year = {2014},\n",
    "               month = {June}\n",
    "}\n",
    "```\n",
    "\n",
    "### Download\n",
    "\n",
    "-. **Images (12.9 GB)**\n",
    "   \n",
    "   http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz\n",
    "-. **Annotations (12.5 MB)**\t\n",
    "   \n",
    "   http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12.tar.gz\n",
    "-. **Videos for each image (25 batches x 17 GB)**\t\n",
    "\n",
    "   http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_sequences_batch1.tar.gz\n",
    "   ...\n",
    "   http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_sequences_batch25.tar.gz\n",
    "-. **Image - video mapping (239 KB)**\t\n",
    "   \n",
    "   http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_sequences_keyframes.mat\n",
    "\n",
    "### Annotation description \n",
    "Annotations are stored in a matlab structure `RELEASE` having following fields\n",
    "\n",
    "- `.annolist(imgidx)` - annotations for image `imgidx`\n",
    "  - `.image.name` - image filename\n",
    "  - `.annorect(ridx)` - body annotations for a person `ridx`\n",
    "\t\t  - `.x1, .y1, .x2, .y2` - coordinates of the head rectangle\n",
    "\t\t  - `.scale` - person scale w.r.t. 200 px height\n",
    "\t\t  - `.objpos` - rough human position in the image\n",
    "\t\t  - `.annopoints.point` - person-centric body joint annotations\n",
    "\t\t    - `.x, .y` - coordinates of a joint\n",
    "\t\t    - `id` - joint id \n",
    "[//]: # \"(0 - r ankle, 1 - r knee, 2 - r hip, 3 - l hip, 4 - l knee, 5 - l ankle, 6 - pelvis, 7 - thorax, 8 - upper neck, 9 - head top, 10 - r wrist, 10 - r wrist, 12 - r shoulder, 13 - l shoulder, 14 - l elbow, 15 - l wrist)\"\n",
    "\t\t    - `is_visible` - joint visibility\n",
    "  - `.vidx` - video index in `video_list`\n",
    "  - `.frame_sec` - image position in video, in seconds\n",
    " \n",
    "- `img_train(imgidx)` - training/testing image assignment \n",
    "- `single_person(imgidx)` - contains rectangle id `ridx` of *sufficiently separated* individuals\n",
    "- `act(imgidx)` - activity/category label for image `imgidx`\n",
    "  - `act_name` - activity name\n",
    "  - `cat_name` - category name\n",
    "  - `act_id` - activity id\n",
    "- `video_list(videoidx)` - specifies video id as is provided by YouTube. To watch video on youtube go to https://www.youtube.com/watch?v=video_list(videoidx) \n",
    "\n",
    "### Browsing the dataset\n",
    "- Please use our online tool for browsing the data\n",
    "http://human-pose.mpi-inf.mpg.de/#dataset\n",
    "- Red rectangles mark testing images\n",
    "\n",
    "### References\n",
    "- **2D Human Pose Estimation: New Benchmark and State of the Art Analysis.**\n",
    "\n",
    "  Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler and Bernt Schiele. \n",
    "\n",
    "  IEEE CVPR'14\n",
    "- **Fine-grained Activity Recognition with Holistic and Pose based Features.**\n",
    "\n",
    "  Leonid Pishchulin, Mykhaylo Andriluka and Bernt Schiele.\n",
    "\n",
    "  GCPR'14\n",
    "\n",
    "### Contact\n",
    "You can reach us via `<lastname>@mpi-inf.mpg.de`\n",
    "We are looking forward to your feedback. If you have any questions related to the dataset please let us know.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since MPII Human Pose Dataset labels are recorded in a MATLAB .mat file, we need to parse it to a clean pandas DataFrame. This format is heavily nested and needs a little bit of exploration to parse it completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .mat file\n",
    "mat = scipy.io.loadmat(MPII_FILE, struct_as_record=False)\n",
    "release_mat = mat['RELEASE'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the `_fieldnames` are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAT _fieldnames ['annolist', 'img_train', 'version', 'single_person', 'act', 'video_list']\n",
      "<Joint 0> X Coordinate of <Person 0> from <Image 4> [[610]]\n"
     ]
    }
   ],
   "source": [
    "print(\"MAT _fieldnames\",release_mat._fieldnames)\n",
    "# Accessing coordinates X of Point 0 from Person 0 in Image 4\n",
    "print(\"<Joint 0> X Coordinate of <Person 0> from <Image 4>\", release_mat.__dict__['annolist'][0][4].annorect[0][0].annopoints[0][0].point[0][0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Label\n",
    "img_train = release_mat.img_train[0]\n",
    "# List of Videos\n",
    "video_list = release_mat.video_list[0]\n",
    "video_list_json = [{'video': {'videoidx':i, 'video_list':item[0]}} for i, item in enumerate(video_list)]\n",
    "# Read Data\n",
    "mpii_version = release_mat.version[0]\n",
    "annolist = release_mat.annolist[0]\n",
    "single_person = release_mat.single_person\n",
    "act = release_mat.act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act List Size 24987\n",
      "Act List Types {<class 'numpy.ndarray'>}\n",
      "Act List Object Types {<class 'scipy.io.matlab._mio5_params.mat_struct'>}\n",
      "Act Fieldnames ['cat_name', 'act_name', 'act_id']\n"
     ]
    }
   ],
   "source": [
    "# Explore Object Length and Type\n",
    "print(\"Act List Size\", len(act))\n",
    "print(\"Act List Types\", {type(i) for i in act})\n",
    "print(\"Act List Object Types\", {type(i[0]) for i in act})\n",
    "# Get _fieldnames\n",
    "print(\"Act Fieldnames\", act[4][0]._fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Act {'act': {'imgidx': 4, 'cat_name': 'sports', 'act_name': ['curling'], 'act_id': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Parse Act\n",
    "act_json = [\n",
    "    {\n",
    "        'act':{\n",
    "            'imgidx':i,\n",
    "            'cat_name':elem[0].cat_name[0] if len(elem[0].cat_name) else None,\n",
    "            'act_name':elem[0].act_name[0].split(', ') if len(elem[0].act_name) else None,\n",
    "            'act_id':elem[0].act_id[0][0]\n",
    "        }\n",
    "    } \n",
    "    for i, elem in enumerate(act)\n",
    "]\n",
    "print(\"Sample Act\", act_json[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Single Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Person List Size 24987\n",
      "Single Person List Types {<class 'numpy.ndarray'>}\n",
      "Single Person List Object Types {<class 'numpy.ndarray'>}\n",
      "Single Person Fieldnames [array([[1],\n",
      "        [2]], dtype=uint8)]\n"
     ]
    }
   ],
   "source": [
    "# Explore Object Length and Type\n",
    "print(\"Single Person List Size\", len(single_person))\n",
    "print(\"Single Person List Types\", {type(i) for i in single_person})\n",
    "print(\"Single Person List Object Types\", {type(i[0][0]) if 0 not in i[0].shape else type(i[0]) for i in single_person})\n",
    "# Get _fieldnames\n",
    "print(\"Single Person Fieldnames\", single_person[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Single Person {'single_person': {'imgidx': 4, 'ridx': [1, 2]}}\n"
     ]
    }
   ],
   "source": [
    "#Parse single_person\n",
    "single_person_json = [\n",
    "    {\n",
    "        'single_person':{\n",
    "            'imgidx':i,\n",
    "            'ridx': [elm[0] for elm in item[0]] if 0 not in item[0].shape else None\n",
    "        }\n",
    "    }\n",
    "    for i, item in enumerate(single_person)\n",
    "]\n",
    "print(\"Sample Single Person\", single_person_json[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Annolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annolist List Size 24987\n",
      "Annolist List Types {<class 'scipy.io.matlab._mio5_params.mat_struct'>}\n",
      "Annolist Fieldnames ['image', 'annorect', 'frame_sec', 'vididx']\n"
     ]
    }
   ],
   "source": [
    "# Explore Object Length and Type\n",
    "print(\"Annolist List Size\", len(annolist))\n",
    "print(\"Annolist List Types\", {type(i) for i in annolist})\n",
    "# Get _fieldnames\n",
    "print(\"Annolist Fieldnames\", annolist[4]._fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annolist Person {'annopoint': {'imgidx': 4, 'image': '015601864.jpg', 'annorect': array([[<scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a878b0>,\n",
      "        <scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a8f370>]],\n",
      "      dtype=object), 'frame_sec': array([11], dtype=uint8), 'vididx': 1660}}\n"
     ]
    }
   ],
   "source": [
    "#Parse annolist\n",
    "annolist_parse_json = [\n",
    "    {\n",
    "        'annopoint':{\n",
    "            'imgidx':i,\n",
    "            'image':item.image[0][0].name[0],\n",
    "            'annorect':item.annorect,\n",
    "            'frame_sec':item.frame_sec[0] if 0 not in item.frame_sec.shape else None,\n",
    "            'vididx':item.vididx[0][0] if 0 not in item.vididx.shape else None,\n",
    "        }\n",
    "    }\n",
    "    for i, item in enumerate(annolist)\n",
    "]\n",
    "print(\"Annolist Person\", annolist_parse_json[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'annopoint': {'imgidx': 2,\n",
       "   'image': '073199394.jpg',\n",
       "   'annorect': array([[<scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a87550>]],\n",
       "         dtype=object),\n",
       "   'frame_sec': None,\n",
       "   'vididx': None}},\n",
       " {'annopoint': {'imgidx': 3,\n",
       "   'image': '059865848.jpg',\n",
       "   'annorect': array([[<scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a87700>]],\n",
       "         dtype=object),\n",
       "   'frame_sec': None,\n",
       "   'vididx': None}},\n",
       " {'annopoint': {'imgidx': 4,\n",
       "   'image': '015601864.jpg',\n",
       "   'annorect': array([[<scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a878b0>,\n",
       "           <scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a8f370>]],\n",
       "         dtype=object),\n",
       "   'frame_sec': array([11], dtype=uint8),\n",
       "   'vididx': 1660}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample with raw parsing\n",
    "annolist_parse_json[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore Annolist Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variables\n",
    "IMAGE_INDEX = 4\n",
    "BODY_INDEX = 0\n",
    "JOINT_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 4 contains 2 person(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a878b0>,\n",
       "       <scipy.io.matlab._mio5_params.mat_struct object at 0x2a2a8f370>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Persons on the IMAGE_INDEX\n",
    "bodies = annolist_parse_json[IMAGE_INDEX]['annopoint']['annorect'][0]\n",
    "print(f\"Image {IMAGE_INDEX} contains {len(bodies)} person(s)\")\n",
    "bodies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annorect structure has the following fields ['x1', 'y1', 'x2', 'y2', 'annopoints', 'scale', 'objpos']\n",
      "<Person 0> from <Image 4> has the following bounding box { x1:627, y1:100, x2:706, y2:198 }\n",
      "<Person 0> from <Image 4> has Scale=3.021046176409755\n",
      "<Person 0> from <Image 4> has ObjPos={ x:594, y:257 }\n",
      "<Person 0> from <Image 4> has 16 joints\n",
      "<Person 0> from <Image 4> Joints= [{'x': 610, 'y': 187, 'id': 6, 'is_visible': array([[0]], dtype=uint8)}, {'x': 647, 'y': 176, 'id': 7, 'is_visible': array([[1]], dtype=uint8)}, {'x': 637.0201, 'y': 189.8183, 'id': 8, 'is_visible': array([], shape=(0, 0), dtype=uint8)}, {'x': 695.9799, 'y': 108.1817, 'id': 9, 'is_visible': array([], shape=(0, 0), dtype=uint8)}, {'x': 620, 'y': 394, 'id': 0, 'is_visible': array([[1]], dtype=uint8)}, {'x': 616, 'y': 269, 'id': 1, 'is_visible': array([[1]], dtype=uint8)}, {'x': 573, 'y': 185, 'id': 2, 'is_visible': array([[1]], dtype=uint8)}, {'x': 647, 'y': 188, 'id': 3, 'is_visible': array([[0]], dtype=uint8)}, {'x': 661, 'y': 221, 'id': 4, 'is_visible': array([[1]], dtype=uint8)}, {'x': 656, 'y': 231, 'id': 5, 'is_visible': array([[1]], dtype=uint8)}, {'x': 606, 'y': 217, 'id': 10, 'is_visible': array([[1]], dtype=uint8)}, {'x': 553, 'y': 161, 'id': 11, 'is_visible': array([[1]], dtype=uint8)}, {'x': 601, 'y': 167, 'id': 12, 'is_visible': array([[1]], dtype=uint8)}, {'x': 692, 'y': 185, 'id': 13, 'is_visible': array([[1]], dtype=uint8)}, {'x': 693, 'y': 240, 'id': 14, 'is_visible': array([[1]], dtype=uint8)}, {'x': 688, 'y': 313, 'id': 15, 'is_visible': array([[1]], dtype=uint8)}]\n"
     ]
    }
   ],
   "source": [
    "# Explore Annorect _fieldnames\n",
    "print(\"Annorect structure has the following fields\", bodies[BODY_INDEX]._fieldnames)\n",
    "# Get Person Bounding Box\n",
    "print(f\"<Person {BODY_INDEX}> from <Image {IMAGE_INDEX}> has the following bounding box {{ x1:{bodies[BODY_INDEX].x1[0][0]}, y1:{bodies[BODY_INDEX].y1[0][0]}, x2:{bodies[BODY_INDEX].x2[0][0]}, y2:{bodies[BODY_INDEX].y2[0][0]} }}\")\n",
    "# Get the scale attribute\n",
    "print(f\"<Person {BODY_INDEX}> from <Image {IMAGE_INDEX}> has Scale={bodies[BODY_INDEX].scale[0][0]}\")\n",
    "# Get the objpos attribute\n",
    "print(f\"<Person {BODY_INDEX}> from <Image {IMAGE_INDEX}> has ObjPos={{ x:{bodies[BODY_INDEX].objpos[0][0].x[0][0]}, y:{bodies[BODY_INDEX].objpos[0][0].y[0][0]} }}\")\n",
    "# Get the joints\n",
    "print(f\"<Person {BODY_INDEX}> from <Image {IMAGE_INDEX}> has {len(bodies[BODY_INDEX].annopoints[0][0].point[0])} joints\")\n",
    "print(f\"<Person {BODY_INDEX}> from <Image {IMAGE_INDEX}> Joints=\", [\n",
    "    dict(x = j.x[0, 0], y=j.y[0, 0], id=j.id[0, 0], is_visible=j.is_visible)\n",
    "    for j in bodies[BODY_INDEX].annopoints[0][0].point[0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse MPII"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package contains utility functions to help you parse the MPII Dataset\n",
    "\n",
    "```python\n",
    "from hourglass_tensorflow.utils.parsers import mpii as mpii_parser\n",
    "\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_mpii\n",
    "\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_objpos\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_is_visible\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_annopoints\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_additional_annorect_item\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_annorect_item\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_annorect\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_annolist\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_img_train\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_single_person\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_video_list\n",
    "from hourglass_tensorflow.utils.parsers.mpii import parse_act\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 3,\n",
       "  'frame_sec': None,\n",
       "  'image': '059865848.jpg',\n",
       "  'vididx': None,\n",
       "  'annorect': [{'index': 0,\n",
       "    'objpos': {'x': 684, 'y': 309},\n",
       "    'scale': 4.928480496055553}]},\n",
       " {'index': 4,\n",
       "  'frame_sec': 11,\n",
       "  'image': '015601864.jpg',\n",
       "  'vididx': 1660,\n",
       "  'annorect': [{'index': 0,\n",
       "    'annopoints': [{'id': 6, 'x': 610, 'y': 187, 'is_visible': 0},\n",
       "     {'id': 7, 'x': 647, 'y': 176, 'is_visible': 1},\n",
       "     {'id': 8, 'x': 637, 'y': 189},\n",
       "     {'id': 9, 'x': 695, 'y': 108},\n",
       "     {'id': 0, 'x': 620, 'y': 394, 'is_visible': 1},\n",
       "     {'id': 1, 'x': 616, 'y': 269, 'is_visible': 1},\n",
       "     {'id': 2, 'x': 573, 'y': 185, 'is_visible': 1},\n",
       "     {'id': 3, 'x': 647, 'y': 188, 'is_visible': 0},\n",
       "     {'id': 4, 'x': 661, 'y': 221, 'is_visible': 1},\n",
       "     {'id': 5, 'x': 656, 'y': 231, 'is_visible': 1},\n",
       "     {'id': 10, 'x': 606, 'y': 217, 'is_visible': 1},\n",
       "     {'id': 11, 'x': 553, 'y': 161, 'is_visible': 1},\n",
       "     {'id': 12, 'x': 601, 'y': 167, 'is_visible': 1},\n",
       "     {'id': 13, 'x': 692, 'y': 185, 'is_visible': 1},\n",
       "     {'id': 14, 'x': 693, 'y': 240, 'is_visible': 1},\n",
       "     {'id': 15, 'x': 688, 'y': 313, 'is_visible': 1}],\n",
       "    'objpos': {'x': 594, 'y': 257},\n",
       "    'scale': 3.021046176409755,\n",
       "    'x1': 627,\n",
       "    'y1': 627,\n",
       "    'x2': 706,\n",
       "    'y2': 706},\n",
       "   {'index': 1,\n",
       "    'annopoints': [{'id': 6, 'x': 979, 'y': 221, 'is_visible': 0},\n",
       "     {'id': 7, 'x': 906, 'y': 190, 'is_visible': 0},\n",
       "     {'id': 8, 'x': 912, 'y': 190},\n",
       "     {'id': 9, 'x': 830, 'y': 182},\n",
       "     {'id': 0, 'x': 895, 'y': 293, 'is_visible': 1},\n",
       "     {'id': 1, 'x': 910, 'y': 279, 'is_visible': 1},\n",
       "     {'id': 2, 'x': 945, 'y': 223, 'is_visible': 0},\n",
       "     {'id': 3, 'x': 1012, 'y': 218, 'is_visible': 1},\n",
       "     {'id': 4, 'x': 961, 'y': 315, 'is_visible': 1},\n",
       "     {'id': 5, 'x': 960, 'y': 403, 'is_visible': 1},\n",
       "     {'id': 10, 'x': 871, 'y': 304, 'is_visible': 1},\n",
       "     {'id': 11, 'x': 883, 'y': 229, 'is_visible': 1},\n",
       "     {'id': 12, 'x': 888, 'y': 174, 'is_visible': 0},\n",
       "     {'id': 13, 'x': 924, 'y': 206, 'is_visible': 1},\n",
       "     {'id': 14, 'x': 1013, 'y': 203, 'is_visible': 1},\n",
       "     {'id': 15, 'x': 955, 'y': 263, 'is_visible': 1}],\n",
       "    'objpos': {'x': 952, 'y': 222},\n",
       "    'scale': 2.472116502109073,\n",
       "    'x1': 841,\n",
       "    'y1': 841,\n",
       "    'x2': 902,\n",
       "    'y2': 902}]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse data with utility functions\n",
    "## `remove_null_keys` is a keyword argument that remove None values when set to True\n",
    "parsed_img_train = mpii_parser.parse_img_train(img_train, remove_null_keys=False)\n",
    "parsed_video_list = mpii_parser.parse_video_list(video_list, remove_null_keys=False)\n",
    "parsed_annolist = mpii_parser.parse_annolist(annolist, remove_null_keys=False)\n",
    "parsed_single_person = mpii_parser.parse_single_person(single_person, remove_null_keys=False)\n",
    "parsed_act = mpii_parser.parse_act(act[0], remove_null_keys=False)\n",
    "# Sample\n",
    "parsed_annolist[3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously parsed arrays are stored as records of generic dictionaries. To make linting and autocompletion available to all, `pydantic.BaseModel` classes are available\n",
    "\n",
    "```python\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAct\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIDataset\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAnnorect\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAnnoPoint\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIDatapoint\n",
    "from hourglass_tensorflow.utils.parsers.mpii import MPIIAnnotation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPIIAnnotation(index=4, annorect=[MPIIAnnorect(index=0, annopoints=[MPIIAnnoPoint(id=6, x=610, y=187, is_visible=0), MPIIAnnoPoint(id=7, x=647, y=176, is_visible=1), MPIIAnnoPoint(id=8, x=637, y=189, is_visible=None), MPIIAnnoPoint(id=9, x=695, y=108, is_visible=None), MPIIAnnoPoint(id=0, x=620, y=394, is_visible=1), MPIIAnnoPoint(id=1, x=616, y=269, is_visible=1), MPIIAnnoPoint(id=2, x=573, y=185, is_visible=1), MPIIAnnoPoint(id=3, x=647, y=188, is_visible=0), MPIIAnnoPoint(id=4, x=661, y=221, is_visible=1), MPIIAnnoPoint(id=5, x=656, y=231, is_visible=1), MPIIAnnoPoint(id=10, x=606, y=217, is_visible=1), MPIIAnnoPoint(id=11, x=553, y=161, is_visible=1), MPIIAnnoPoint(id=12, x=601, y=167, is_visible=1), MPIIAnnoPoint(id=13, x=692, y=185, is_visible=1), MPIIAnnoPoint(id=14, x=693, y=240, is_visible=1), MPIIAnnoPoint(id=15, x=688, y=313, is_visible=1)], objpos=MPIIObjPos(x=594, y=257), scale=3.021046176409755, x1=627, y1=627, x2=706, y2=706, head_r11=None, head_r12=None, head_r13=None, head_r21=None, head_r22=None, head_r23=None, head_r31=None, head_r32=None, head_r33=None, part_occ1=None, part_occ10=None, part_occ2=None, part_occ3=None, part_occ4=None, part_occ5=None, part_occ6=None, part_occ7=None, part_occ8=None, part_occ9=None, torso_r11=None, torso_r12=None, torso_r13=None, torso_r21=None, torso_r22=None, torso_r23=None, torso_r31=None, torso_r32=None, torso_r33=None), MPIIAnnorect(index=1, annopoints=[MPIIAnnoPoint(id=6, x=979, y=221, is_visible=0), MPIIAnnoPoint(id=7, x=906, y=190, is_visible=0), MPIIAnnoPoint(id=8, x=912, y=190, is_visible=None), MPIIAnnoPoint(id=9, x=830, y=182, is_visible=None), MPIIAnnoPoint(id=0, x=895, y=293, is_visible=1), MPIIAnnoPoint(id=1, x=910, y=279, is_visible=1), MPIIAnnoPoint(id=2, x=945, y=223, is_visible=0), MPIIAnnoPoint(id=3, x=1012, y=218, is_visible=1), MPIIAnnoPoint(id=4, x=961, y=315, is_visible=1), MPIIAnnoPoint(id=5, x=960, y=403, is_visible=1), MPIIAnnoPoint(id=10, x=871, y=304, is_visible=1), MPIIAnnoPoint(id=11, x=883, y=229, is_visible=1), MPIIAnnoPoint(id=12, x=888, y=174, is_visible=0), MPIIAnnoPoint(id=13, x=924, y=206, is_visible=1), MPIIAnnoPoint(id=14, x=1013, y=203, is_visible=1), MPIIAnnoPoint(id=15, x=955, y=263, is_visible=1)], objpos=MPIIObjPos(x=952, y=222), scale=2.472116502109073, x1=841, y1=841, x2=902, y2=902, head_r11=None, head_r12=None, head_r13=None, head_r21=None, head_r22=None, head_r23=None, head_r31=None, head_r32=None, head_r33=None, part_occ1=None, part_occ10=None, part_occ2=None, part_occ3=None, part_occ4=None, part_occ5=None, part_occ6=None, part_occ7=None, part_occ8=None, part_occ9=None, torso_r11=None, torso_r12=None, torso_r13=None, torso_r21=None, torso_r22=None, torso_r23=None, torso_r31=None, torso_r32=None, torso_r33=None)], frame_sec=11, image='015601864.jpg', vididx=1660)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use one BaseModel to make parsed_annolist as a record of structured object\n",
    "structured_annolist = [MPIIAnnotation.parse_obj(an) for an in parsed_annolist]\n",
    "structured_annolist[4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line Interface (HTF)\n",
    "\n",
    "To make use of these utility functions, a command line interface (CLI) is available when installing the `hourglass_tensorflow` package\n",
    "\n",
    "```bash\n",
    "$ htf [OPTIONS] COMMAND [ARGS]\n",
    "$ htf mpii --help # For MPII related operations\n",
    "$ htf mpii parse   ... # See documentation\n",
    "$ htf mpii convert ... # See documentation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: htf [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  mpii  Operation related to MPII management / parsing\n"
     ]
    }
   ],
   "source": [
    "!htf --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: htf mpii [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Operation related to MPII management / parsing\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  convert  Convert a MPII .mat file to a HTF compliant record\n",
      "  parse    Parse a MPII .mat file to a more readable record\n"
     ]
    }
   ],
   "source": [
    "!htf mpii --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: htf mpii convert [OPTIONS] INPUT OUTPUT\n",
      "\n",
      "  Convert a MPII .mat file to a HTF compliant record\n",
      "\n",
      "Options:\n",
      "  -v, --verbose / --no-verbose  Activate Logs\n",
      "  --help                        Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!htf mpii convert --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: htf mpii parse [OPTIONS] INPUT OUTPUT\n",
      "\n",
      "  Parse a MPII .mat file to a more readable record\n",
      "\n",
      "Options:\n",
      "  -v, --verbose / --no-verbose  Activate Logs\n",
      "  --validate / --no-validate    Whether to use validation checks (default\n",
      "                                false)\n",
      "  --struct / --no-struct        Whether or not to apply pydantic parsing\n",
      "                                (default false)\n",
      "  --as-list / --no-as-list      Activate to return list of records (default\n",
      "                                false)\n",
      "  --null / --no-null            Keep null values in records (default true)\n",
      "  --help                        Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!htf mpii parse --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('hourglass-tensorflow-Pl5p0fL5-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be2d5d21ec23e1ab010cf74c72177a0b2359c60c6692a150b0c617aebb7cf1ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
